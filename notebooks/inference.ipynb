{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "(parent_folder_path, current_dir) = os.path.split(os.path.abspath(''))\n",
    "sys.path.append(parent_folder_path)\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import os\n",
    "# import pickle\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "\n",
    "from equities.model import GPTConfig, GPT\n",
    "from equities.data_processing import itch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT PARAMS\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = parent_folder_path + '/out' # ignored if init_from is not 'resume'\n",
    "dataset = '12302019.NASDAQ_ITCH50_AAPL_message_proc.npy' # dataset to use for initial prompt\n",
    "# start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_context_msgs = 3 # number of messages from dataset to use as context\n",
    "# num_samples = 10 # number of samples to draw\n",
    "num_samples = 1 # number of samples to draw\n",
    "# max_new_tokens = 500 # number of tokens generated in each sample\n",
    "max_new_tokens = 1 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 42\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# exec(open('equities/configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 94.57M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(12515, 768)\n",
       "    (wpe): Embedding(10367, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=12515, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_raw.shape: (3, 18)\n",
      "X_raw: [[      40  7872401        1        0    27499     -999      100    -9999\n",
      "         0 15161946    34200 22603012    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40  7872405        1        0    28368     -595      100    -9999\n",
      "         0     6899    34200 22609911    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40  7872421        1        1    30400      999      100    -9999\n",
      "         0    16782    34200 22626693    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]]\n",
      "X.shape: (3, 24)\n",
      "X: [[12051  1003 12010 12008 12007  1108     2     3    18   164   949    37\n",
      "    203    25   606    15     2     2     2     2     2     2     2     2]\n",
      " [12051  1003 12010 12008 11603  1108     2     3     3     9   902    37\n",
      "    203    25   612   914     2     2     2     2     2     2     2     2]\n",
      " [12051  1003 12011 12009 12007  1108     2     3     3    19   785    37\n",
      "    203    25   629   696     2     2     2     2     2     2     2     2]]\n",
      "decoded X: [[      40    -9999        1        0    -9999     -999      100    -9999\n",
      "         0 15161946    34200 22603012    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40    -9999        1        0    -9999     -595      100    -9999\n",
      "         0     6899    34200 22609911    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40    -9999        1        1    -9999      999      100    -9999\n",
      "         0    16782    34200 22626693    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]]\n",
      "['ticker', 'NA_VAL', 'event_type', 'direction', 'NA_VAL', 'price', 'fill_size', 'remain_size', 'delta_t_s', 'delta_t_ns', 'time_s', 'time_ns', 'NA_VAL', 'price_ref', 'fill_size_ref', 'time_s_ref', 'time_ns_ref', 'NA_VAL']\n"
     ]
    }
   ],
   "source": [
    "# define path for sample data\n",
    "# dataset = '12302019.NASDAQ_ITCH50_AAPL_message_proc.npy'\n",
    "data_dir = os.path.join('dataset/proc/ITCH/test/', dataset)\n",
    "data_dir = parent_folder_path + '/' + data_dir\n",
    "\n",
    "# grab sample data to use as context\n",
    "context_dataset = np.load(data_dir, mmap_mode='r')\n",
    "X_raw = np.array(context_dataset[0:num_context_msgs])\n",
    "print(\"X_raw.shape:\", X_raw.shape)\n",
    "print(\"X_raw:\", X_raw)\n",
    "\n",
    "# encode the sample data\n",
    "vocab = itch_encoding.Vocab()\n",
    "X = itch_encoding.encode_msgs(X_raw, vocab.ENCODING)\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"X:\", X)\n",
    "\n",
    "# decode the sample data (will be missing order id, price_abs, old_id, and old_price_abs)\n",
    "print(\"decoded X:\", itch_encoding.decode_msgs(X, vocab.ENCODING))\n",
    "print([ \"ticker\", \"NA_VAL\",\n",
    "        \"event_type\", \"direction\", \"NA_VAL\", \"price\", \"fill_size\", \"remain_size\",\n",
    "        \"delta_t_s\", \"delta_t_ns\", \"time_s\", \"time_ns\",\n",
    "        \"NA_VAL\", \"price_ref\", \"fill_size_ref\", \"time_s_ref\", \"time_ns_ref\", \"NA_VAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_tok_len: 24\n"
     ]
    }
   ],
   "source": [
    "encoded_tok_len = X.shape[1]\n",
    "print(\"encoded_tok_len:\", encoded_tok_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 72])\n",
      "x: tensor([[12051,  1003, 12010, 12008, 12007,  1108,     2,     3,    18,   164,\n",
      "           949,    37,   203,    25,   606,    15,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2, 12051,  1003, 12010, 12008, 11603,  1108,\n",
      "             2,     3,     3,     9,   902,    37,   203,    25,   612,   914,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2, 12051,  1003,\n",
      "         12011, 12009, 12007,  1108,     2,     3,     3,    19,   785,    37,\n",
      "           203,    25,   629,   696,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2]], device='cuda:0')\n",
      "generated token: 2\n",
      "---------------\n",
      "new sequence [12051, 1003, 12010, 12008, 12007, 1108, 2, 3, 18, 164, 949, 37, 203, 25, 606, 15, 2, 2, 2, 2, 2, 2, 2, 2, 12051, 1003, 12010, 12008, 11603, 1108, 2, 3, 3, 9, 902, 37, 203, 25, 612, 914, 2, 2, 2, 2, 2, 2, 2, 2, 12051, 1003, 12011, 12009, 12007, 1108, 2, 3, 3, 19, 785, 37, 203, 25, 629, 696, 2, 2, 2, 2, 2, 2, 2, 2, 12051, 1003, 12010, 12008, 11016, 1108, 2, 3, 3, 3, 3, 37, 584, 23, 550, 755, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# prepare context tensor\n",
    "x = (torch.tensor(X.reshape(-1), dtype=torch.long, device=device)[None, ...])\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"x:\", x)\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            y = model.generate(x, max_new_tokens*encoded_tok_len, temperature=temperature, top_k=top_k)\n",
    "            # print(decode(y[0].tolist()))\n",
    "            print(\"last generated msg:\", y[0][-1].tolist())\n",
    "            # print(y[0].tolist())\n",
    "            print('---------------')\n",
    "        print(\"new sequence\", y[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last generated msg: [12051, 1003, 12010, 12008, 11016, 1108, 2, 3, 3, 3, 3, 37, 584, 23, 550, 755, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "y: tensor([[12051,  1003, 12010, 12008, 12007,  1108,     2,     3,    18,   164,\n",
      "           949,    37,   203,    25,   606,    15,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2, 12051,  1003, 12010, 12008, 11603,  1108,\n",
      "             2,     3,     3,     9,   902,    37,   203,    25,   612,   914,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2, 12051,  1003,\n",
      "         12011, 12009, 12007,  1108,     2,     3,     3,    19,   785,    37,\n",
      "           203,    25,   629,   696,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2, 12051,  1003, 12010, 12008, 11016,  1108,     2,     3,\n",
      "             3,     3,     3,    37,   584,    23,   550,   755,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2]], device='cuda:0')\n",
      "decoded msg: [      40    -9999        1        0    -9999       -8      100    -9999\n",
      "        0        0    34581 20547752    -9999    -9999    -9999    -9999\n",
      "    -9999    -9999]\n",
      "['ticker', 'NA_VAL', 'event_type', 'direction', 'NA_VAL', 'price', 'fill_size', 'remain_size', 'delta_t_s', 'delta_t_ns', 'time_s', 'time_ns', 'NA_VAL', 'price_ref', 'fill_size_ref', 'time_s_ref', 'time_ns_ref', 'NA_VAL']\n"
     ]
    }
   ],
   "source": [
    "# print the last message in the generated sequence\n",
    "print(\"last generated msg:\", y[0][-24:].tolist())\n",
    "\n",
    "# print(y[0].tolist())\n",
    "print(\"y:\", y)\n",
    "\n",
    "# decode the generated sequence\n",
    "# print(\"decoded y:\", itch_encoding.decode_msg(y[0][-24:].tolist(), vocab.ENCODING))\n",
    "print(\"decoded msg:\", itch_encoding.decode_msg(np.array(y[0][-24:].tolist()), vocab.ENCODING))\n",
    "print([ \"ticker\", \"NA_VAL\",\n",
    "        \"event_type\", \"direction\", \"NA_VAL\", \"price\", \"fill_size\", \"remain_size\",\n",
    "        \"delta_t_s\", \"delta_t_ns\", \"time_s\", \"time_ns\",\n",
    "        \"NA_VAL\", \"price_ref\", \"fill_size_ref\", \"time_s_ref\", \"time_ns_ref\", \"NA_VAL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_true: [[      40  7872401        1        0    27499     -999      100    -9999\n",
      "         0 15161946    34200 22603012    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40  7872405        1        0    28368     -595      100    -9999\n",
      "         0     6899    34200 22609911    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40  7872421        1        1    30400      999      100    -9999\n",
      "         0    16782    34200 22626693    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]\n",
      " [      40  7872457        1        1    29532      569      100    -9999\n",
      "         0    10618    34200 22637311    -9999    -9999    -9999    -9999\n",
      "     -9999    -9999]]\n",
      "true last msg: [      40  7872457        1        1    29532      569      100    -9999\n",
      "        0    10618    34200 22637311    -9999    -9999    -9999    -9999\n",
      "    -9999    -9999]\n"
     ]
    }
   ],
   "source": [
    "X_true = np.array(context_dataset[0:num_context_msgs+max_new_tokens])\n",
    "print(\"X_true:\", X_true)\n",
    "\n",
    "print(\"true last msg:\", X_true[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarketSimT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
